### MySQL架构

![image-20210408192124960](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210408192124960.png)



### ICP索引下推

MySQL在取出索引数据的同时，判断是否可以进行where条件过滤，将where的部分过滤操作放在存储引擎层提前过滤掉不必要的数据，减少了不必要数据被扫描带来的IO开销。

![image-20210316102137899](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210316102137899.png)

开启ICP特性后，由于 nickname 的 like 条件可以通过索引筛选，存储引擎层通过索引与 where 条件的比较来去除不符合条件的记录，这个过程不需要读取记录，同时只返回给Server层筛选后的记录，减少不必要的IO开销。

**Extra显示的索引扫描方式**

- using where：查询使用索引的情况下，需要回表去查询所需的数据。索引全扫描
- using index condition：查询使用了索引，但是需要回表查询数据。
- using index：查询使用覆盖索引的时候会出现。不需要回表
- using index & using where：查询使用了索引，但是需要的数据都在索引列中能找到，不需要回表查询数据。

### 模糊匹配优化

`alter table users01 add index idx_nickname(nickname);`

`select * from users01 where nickname like '%SK%';`

从执行计划看到 type=ALL，Extra=Using where 走的是全部扫描，没有利用到ICP特性。

辅助索引idx_nickname(nickname)内部是包含主键id的，等价于(id，nickname)的复合索引

尝试利用覆盖索引特性将SQL改写为 `select Id from users01 where nickname like '%SK%'` 。

从执行计划看到，`type=index，Extra=Using where; Using index`，索引全扫描，但是需要的数据都在索引列中能找到，不需要回表。

利用这个特点，将原始的SQL语句先获取主键id，然后通过id跟原表进行关联，分析其执行计划：

`select * from users01 a , (select id from users01 where nickname like '%SK%') b where a.id = b.id;`

从执行计划看，走了索引idx_nickname，不需要回表访问数据，执行时间从60ms降低为40ms，type = index 说明没有用到ICP特性

但是可以利用 `Using where; Using index` 这种索引扫描不回表的方式减少资源开销来提升性能。

### 全文索引

![image-20210313145453225](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313150900962.png)

`explain select * from users01 where match(nickname) against('看风');`

使用了全文索引的方式查询，type=fulltext，同时命中全文索引 `idx_full_nickname`，从上面的分析可知，在MySQL中，对于完全模糊匹配%%查询的SQL可以通过全文索引提高效率。

### MySQL调优

![image-20210314141622799](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313152314171.png)

* 排除缓存干扰：第一次sql缓存失效，更新后第一次查询会查不到缓存。执行SQL的时候，记得加上SQL NoCache去跑SQL，这样跑出来的时间就是真实的查询时间了。8.0以上版本没有缓存干扰问题
* Explain：![image-20210313150900962](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313145453225.png)

* 采用统计：InnoDB默认会选择N个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。在实践中，如果你发现explain的结果预估的rows值跟实际情况差距比较大，可以采用analyze table tablename来处理。

* 覆盖索引：如果在我们建立的索引上就已经有我们需要的字段，就不需要回表了，在电商里面也是很常见的，我们需要去商品表通过各种信息查询到商品id，id一般都是主键。`select itemId from itemCenter where size between 1 and 6`，因为商品id itemId一般都是主键，在size索引上肯定会有我们这个值，这个时候就不需要回主键表去查询id信息了。

* 联合索引：建立一个名称和库存的联合索引，这样名称查出来就可以看到库存了，不需要查出id之后去回表再查询库存了

* 最左匹配原则：` itemname like ’谢%‘`
* 索引下推：`select * from itemcenter where name like '敖%' and size=22 and age = 20;`即在引擎层就通过条件帅选出数据![image-20210313152314171](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210316101816964.png)

* 唯一索引普通索引选择：change buffer
* change buffer：
  * 当需要更新一个数据页时，如果数据页在内存中就直接更新（唯一索引），而如果这个数据页还没有在内存中的话（普通索引），在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。             
  * **merge**过程：在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作，通过这种方式就能保证这个数据逻辑的正确性。系统有**后台线程会定期merge**。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。
  * change buffer在内存中有拷贝，也会被写入到磁盘上。
  * 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好，这种业务模型常见的就是账单类、日志类的系统。
  * ![image-20210313155237392](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313155237392.png)

* 前缀索引：**区分度很高**，可以使用倒序、REVERSE()反转、删减字符串、substring截取字符串等，但调用函数也需要开销，

* 条件字段函数的操作：**对索引字段做函数操作**，可能会**破坏索引值的有序性**，因此优化器就决定放弃走树搜索功能。

* 隐式函数转换：`select * from t where id = 1`  ，如果id是字符类型的，1是数字类型的，你用explain会发现走了全表扫描，根本用不上索引。MySQL底层会对你的比较进行转换，相当于加了 CAST( id AS signed int) 这样的一个函数，函数会导致走不上索引。
* 隐式字符编码转换：如果两个表的字符集不一样，一个是utf8mb4，一个是utf8，因为utf8mb4是utf8的超集，一旦两个字符比较，就会转换为utf8mb4再比较。转换的过程相当于加了CONVERT(id USING utf8mb4)函数

* flush：redo log大家都知道，也就是我们对数据库操作的日志，他是在内存中的，每次操作一旦写了redo log就会立马返回结果，但是这个redo log总会找个时间去更新到磁盘，这个操作就是flush。
  * 内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页“。
  * InnoDB的redo log写满了，这时候系统会停止所有更新操作，把checkpoint往前推进，redo log留出空间可以继续写。
  * 系统内存不足，当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。
  * ![image-20210313155413495](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313155413495.png)

### InnoDB

* 分为两大块：InnoDB In-Memory Structures 和 InnoDB On-Disk Structures，即缓存和磁盘
* 缓存：
  * Buffer Pool 是`a linked list of pages`，一个以页为元素的链表。采用基于 LRU（least recently used） 的算法来管理内存：
  * change Buffer：如果 MySQL 发现你要修改的页，不在内存里，就把你要对页的修改，先记到一个叫 Change Buffer 的地方，同时记录 redo log，然后再慢慢把数据 load 到内存，load 过来后，再把 Change Buffer 里记录的修改，应用到内存（Buffer Pool）中，这个动作叫做 **merge**；而把内存数据刷到磁盘的动作，叫 **purge**：
  * **Adaptive Hash Index**自适应哈希索引：MySQL 会自动评估使用自适应索引是否值得，如果观察到建立哈希索引可以提升速度，则建立
  * Log Buffer：Log Buffer 里的 redo log，会被刷到磁盘里：
* 磁盘：平时创建的表的数据，可以存放到 The System Tablespace 、File-Per-Table Tablespaces、General Tablespace 三者中的任意一个地方
  * Doublewrite Buffer：**Change Buffer 是提升性能，那么 Doublewrite Buffer 就是保证数据页的可靠性。**MySQL 在刷数据到磁盘之前，要先把数据写到另外一个地方，也就是 Doublewrite Buffer

![image-20210314142140004](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313203143012.png)

* 采用 MVCC 来支持高并发，并且实现了四个标准隔离级别(未提交读、提交读、可重复读、可串行化)。其默认级别时可重复读（REPEATABLE READ），在可重复读级别下，通过 MVCC + Next-Key Locking 防止幻读。
* InnoDB 内部做了很多优化，包括从磁盘读取数据时采用的可预测性读，能够自动在内存中创建 hash 索引以加速读操作的自适应哈希索引，以及能够加速插入操作的插入缓冲区等。
* InnoDB 支持真正的在线热备份，即读取一致性视图不需要停止表的读写，而MySQL 其他的存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合的场景中，停止写入可能也意味着停止读取。
* 数据页：![image-20210313203143012](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210314142140004.png)![image-20210313203225408](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313202916258.png)

### MyISAM

* 提供了大量的特性，包括压缩表、空间数据索引等。不支持事务。
* 不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。
* 在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。

* InnoDB 和 MyISAM 的比较
  - 事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。
  - 并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。
  - 外键：InnoDB 支持外键。
  - 备份：InnoDB 支持在线热备份。
  - 崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。
  - 其它特性：MyISAM 支持压缩表和空间数据索引。

### 索引 

* B+ Tree：**B+ Tree 是 B 树的一种变形，它是基于 B Tree 和叶子节点顺序访问指针进行实现，通常用于数据库和操作系统的文件系统中。**

* B+ 树有两种类型的节点：**内部节点（也称索引节点）和叶子节点**，内部节点就是非叶子节点，内部节点不存储数据，**只存储索引**，数据都存在叶子节点。内部节点中的 key 都按照**从小到大**的顺序排列，对于内部节点中的一个 key，左子树中的所有 key 都小于它，右子树中的 key 都大于等于它，**叶子节点的记录也是按照从小到大排列的。**

* ![image-20210328114615174](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210328114615174.png)

* ![image-20210313202916258](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313220605753.png)

* 查找：查找以典型的方式进行，类似于二叉查找树。起始于根节点，自顶向下遍历树，选择其分离值在要查找值的任意一边的子指针。在节点内部典型的使用是二分查找来确定这个位置。

* 删除：和插入类似，只不过是自下而上的合并操作。

* 红黑树：通过对从根节点到叶子节点路径上各个节点的颜色进行约束，确保没有一条路径会比其他路径长2倍，因而是近似平衡的。所以相对于严格要求平衡的AVL树来说，它的旋转保持平衡次数较少。适合，查找少，插入/删除次数多的场景。

* AVL数：平衡二叉树，一般是**用平衡因子差值决定并通过旋转**来实现，左右子树树**高差不超过1**，那么和红黑树比较它是严格的平衡二叉树，平衡条件非常严格（树高差只有1），只要插入或删除不满足上面的条件就要通过旋转来保持平衡。由于**旋转是非常耗费时间**的。所以 AVL 树适用于插入/删除次数比较少，但查找多的场景。

* B/B+数：多路查找树，出度高，磁盘IO低，一般用于数据库系统中。

* B-数：B类树是平衡树，每个结点到叶子结点的高度都是相同，这也保证了每个查询是稳定的，查询的时间复杂度时long2(n)；

* B+-树和B-树的比较

  * （1）B+树更适合外部存储(一般指磁盘存储),由于内节点(非叶子节点)不存储data，所以一个节点可以存储更多的内节点，每个节点能索引的范围更大更精确。也就是说使用B+树单次磁盘IO的信息量相比较B树更大，IO效率更高。

  * （2）mysql是关系型数据库，经常会按照区间来访问某个索引列，B+树的叶子节点间按顺序建立了链指针，加强了区间访问性，所以B+树对索引列上的区间范围查询很友好。而B树每个节点的key和data在一起，无法进行区间查找。

    （3）B+树的查询效率更加稳定：由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。

* B+数和红黑树比较（即为什么用B+树）：

  * **磁盘局部性原理**：

    * 为了提升效率，要尽量减少磁盘IO的次数。实际过程中，磁盘并不是每次严格按需读取，而是每次都会预读。磁盘读取完需要的数据后，会**按顺序再多读一部分数据到内存**中，这样做的理论依据是计算机科学中注明的局部性原理：

      1. 当一个数据被用到时，其**附近的数据也通常会马上被使用**

      2. **程序运行期间所需要的数据通常比较集中**

      （1）由于磁盘顺序读取的效率很高(不需要寻道时间，只需很少的旋转时间)，

      因此对于具有局部性的程序来说，预读可以提高I/O效率.  **预读的长度一般为页(page)的整倍数。**

      （2）MySQL(默认使用InnoDB引擎),将记录按照页的方式进行管理,**每页大小默认为16K**(这个值可以修改)。**linux 默认页大小为4K。**

  * （一）磁盘 **IO 次数**

    B+ 树**一个节点可以存储多个元素**，相对于红黑树的**树高更低，磁盘 IO 次数更少**。

  * （二）磁盘**预读特性**

    * 为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会**预读**。预读过程中，磁盘进行**顺序读取**，顺序读取不需要进行**磁盘寻道**。每次会**读取页的整数倍**。

    * 操作系统一般**将内存和磁盘分割成固定大小的块**，每一块称为一页，内存与磁盘以**页为单位**交换数据。数据库系统**将索引的一个节点的大小设置为页的大小**，使得**一次 I/O 就能完全载入一个节点。**
    * 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个结点只需一次I/O。
    * 而红黑树这种结构，高度H明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(H)，效率明显比B-Tree差很多。
    * 假设 B-Tree 的高度为 h,B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3，也即索引的B+树层次一般不超过三层，所以查找效率很高）

  * 因为不再需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多。

  * 因为 B+ Tree 的**有序性**，所以除了用于查找，还可以用于**排序和分组**。

  * 可以指定**多个列作为索引列**，**多个索引列共同组成键。**

* 各种数的比较

  * **B树：**二叉树，每个结点只存储一个关键字，等于则命中，小于走左结点，大于走右结点；

    **B-树：**多路搜索树，每个结点存储M/2到M个关键字，非叶子结点存储指向关键字范围的子结点；所有关键字在整颗树中出现，且只出现一次，非叶子结点可以命中；![image-20210330220853039](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210330220853039.png)

    **B+树：**在B-树基础上，为叶子结点增加链表指针，所有关键字都在叶子结点中出现，非叶子结点作为叶子结点的索引；B+树总是到叶子结点才命中；

    **B\*树：** 在B+树基础上，为非叶子结点也增加链表指针，将结点的最低利用率从1/2提高到2/3；

* 适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。

* InnoDB 的 B+Tree 索引分为**主索引和辅助索引**。主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找，这个过程也被称作**回表**。

* 哈希索引：哈希索引能以 **O(1) 时间**进行查找，但是失去了有序性：
  - 无法用于排序与分组；
  - 只支持精确查找，无法用于部分查找和范围查找。

* 自哈希索引：当某个索引值被使用的非常频繁时，会**在 B+Tree 索引之上再创建一个哈希索引**，这样就让 B+Tree 索引具有哈希索引的一些优点，比如**快速的哈希查找**。

* 全文索引：查找条件使用 `MATCH AGAINST`，而不是普通的 WHERE。使用**倒排索引**实现，它**记录着关键词到其所在文档的映射**。

* 空间数据索引：MyISAM 存储引擎支持空间数据索引**（R-Tree）**，可以用于**地理数据存储**。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。必须使用 GIS 相关的函数来维护数据。

### 索引优化

* 单列索引：在进行查询时，索引列不能是表达式的一部分，也不能是函数的参数，否则无法使用索引，如 `SELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5;`
* 多列索引：在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好。
* 索引顺序：让选择性（重复性小）最强的索引列放在前面。
* 前缀索引：对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符。前缀长度的选取需要根据索引选择性来确定。
* 覆盖索引：索引包含所有需要查询的字段的值。
* 索引优点：帮助服务器避免进行排序和分组，以及避免创建临时表（B+Tree 索引是有序的，可以用于 ORDER BY 和 GROUP BY 操作。临时表主要是在排序和分组过程中创建，不需要排序和分组，也就不需要创建临时表）。将随机 I/O 变为顺序 I/O（B+Tree 索引是有序的，会将相邻的数据都存储在一起）。
* 索引使用条件：对于中到大型的表，索引就非常有效；但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。

* 索引失效的情况：

  * **range类型查询字段后面的索引无效**。解决方法可以是：**range类型的字段不要建立索引**，这样range后面的SQL语句能生效

  * 不在索引列上做任何操作（**计算、函数、（自动or手动）类型转换**），会导致索引失效而转向全表扫描
    尽量使用覆盖索引（只访问索引的查询（索引列和查询列一致）），减少select *
    mysql在使用 **!= 、<、> **的时候无法使用索引会导致全表扫描

    **is not null**也无法使用索引，is null可以使用索引

  * **字符串不加单引号**索引失效  

    like以通配符开头（’%abc…’）mysql索引失效会变成全表扫描的操作。问题：解决like‘%字符串%’时索引不被使用的方法？

    * 采用覆盖索引解决，先建立name索引后：`SELECT id,name from tbl_user WHERE name like '%aa%'`，即要查询的列都在索引中

* 单列、组合索引建立：

  * 对于单键索引，尽量选择针对当前**query过滤性** (值相同最少的列) 更好的索引
    在选择组合索引的时候，当前Query中过滤性最好的字段在索引字段顺序中，**位置越靠前越好**。
    在选择组合索引的时候，尽量选择可以能够包含当前query中的**where子句中更多字段的索引**
    尽可能通过分析统计信息和调整query的写法来达到选择合适索引的目的

* 最佳左前缀原则：查询条件中出现联合索引第一列,或者全部,则能利用联合索引.

  * 最左优先，以最左边的为起点任何连续的索引都能匹配上。同时遇到范围查询(>、<、between、like)就会停止匹配。

    *  假如建立联合索引（a,b,c），用到了索引的：

      * `select * from table_name where b = '2' and a = '1' and c = '3'`。用到了索引，where子句几个搜索条件顺序调换不影
      * `select * from table_name where a = '1' and b = '2' `  匹配左边的列，索引有效

    * 没用到索引：

      * `select * from table_name where  b = '1' and c = '3'`  ，没有从最左边开始，是全表扫描
      * `select * from table_name where a = '1' and c = '3' `， abc没有都用到，即查询条件不连续，只用到了a列的索引，b列和c列都没有用到 

    * 匹配列前缀：

      ```
      select * from table_name where a like 'As%'; //前缀都是排好序的，走索引查询
      select * from table_name where  a like '%As'//全表查询，可以用覆盖索引解决
      ```

* EXISTS和IN
  
* ![image-20210331192627840](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210331192627840.png)
  
* ORDER BY 和 GROUP BY
  * Mysql支持**2种方式的排序**，**FileSort**和**Index**，Index效率高，指Mysql**扫描索引本身完成排序**，**FileSort效率低** 
    * 两种fileSort算法：
      * **双路排序**：Mysql4.1之前使用双路排序，扫描**两次磁盘**
      * **单路排序 (随机IO变为顺序IO)**：**磁盘读取查询需要的所有列**，在内存中**按照 order by列在sort_buffer对它们进行排序**，然后**扫描排序后的列表进行输出**。**避免了第二次读取数据**。并且把**随机O变成了顺序**，但是它会**使用更多的空间**，因为它把每一行都**保存在内存**中了
  * **WHERE 和 ORDER BY语句所使用索引作为一个整体，满足最左前列原则**，索引就有效
  * **where 高于having**，能写where限定的条件就不去having限定
* 慢查询解决：
  * 慢查询 ( **设置查询时间阈值并抓取慢sql** ) 的**开启并捕获**
  * **explain + 慢SQL分析**
  * **show profile** 查询SQL在Mysql服务器里面**执行细节和生命周期情况**
  * SQL**数据库服务器的参数调优**，日志分析工具：**mysqldumpslow**

* explain
  * select_type：常用的有 SIMPLE 简单查询，UNION 联合查询，SUBQUERY 子查询等。
  * table：要查询的表
  * possible_keys：可选择的索引
  * key：实际使用的索引
  * rows：扫描的行数
  * type：索引查询类型，经常用到的索引查询类型：
    * system:  表只有一行，这是一个 const type 的特殊情况
    * const：使用主键或者唯一索引进行查询的时候只有一行匹配 
    * eq_ref：在进行联接查询的，使用主键或者唯一索引并且只匹配到一行记录的时候
    * ref：使用非唯一索引 
    * range：使用主键、单个字段的辅助索引、多个字段的辅助索引的最后一个字段进行范围查询 
    * index：和all的区别是扫描的是索引树 。1）查询的字段是索引的一部分，覆盖索引。2）使用主键进行排序
    * all：扫描全表：

* 减少请求的数据量：
  * 只返回必要的列：最好不要使用 SELECT * 语句。
  * 只返回必要的行：使用 LIMIT 语句来限制返回的数据。
  * 缓存重复查询的数据：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。
* 减少服务器端扫描的行数：最有效的方式是使用索引来覆盖查询，即覆盖索引

* 切分大查询：一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。
* 分解大连接查询：将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联，

### Buffer Poll

![image-20210408202953947](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210408202953947.png)

* **缓冲池**(buffer pool)机制，以避免每次查询数据都进行磁盘IO。

* 缓存表数据与索引数据，把磁盘上的数据加载到缓冲池，避免每次访问都进行磁盘IO，起到加速访问的作用。

* 预读：

  * 磁盘读写，并不是按需读取，而是按页读取，一次至少读一页数据（一般是 **4K**），如果未来要读取的数据就在页中，就能够省去后续的磁盘IO，提高效率。

  * 数据访问，通常都遵循“集中读写”的原则，使用一些数据，**大概率会使用附近的数据**，这就是所谓的“**局部性原理**”，它表明提前加载是有效的，确实能够减少磁盘IO。

  * （1）磁盘访问按页读取能够提高性能，所以**缓冲池一般也是按页缓存数据；**

    （2）预读机制启示了我们，能把一些**“可能要访问”的页提前加入缓冲池**，避免未来的磁盘IO操作；

* 实际上默认情况下，磁盘中存放的数据页的大小是**16KB**，也就是说，一页数据包含了16KB的内容。 而Buffer Pool中存放的一个一个的数据页，我们通常叫做缓存页，因为毕竟Buffer Pool是一个缓冲池，里面的数据都是从磁盘缓存到内存去的。 而Buffer Pool中默认情况下，一个缓存页的大小和磁盘上的一个数据页的大小是一一对应起来的，都是16KB。 

* Buffer Pool本质其实就是数据库的一个内存组件，你可以理解为他就是一片内存数据结构，所以这个内存数据结构肯定是有一定的大小的，不可能是无限大的。 这个Buffer Pool默认情况下是128MB。一般是给buffer pool设置你的机器内存的50%~60%左右 

* 缓冲池污染：当某一个SQL语句，要批量扫描大量数据时，可能导致把缓冲池的所有页都替换出去，导致大量热数据被换出，MySQL性能急剧下降，这种情况叫缓冲池污染。MySQL缓冲池加入了一个“老年代停留时间窗口”的机制：

* 加入“老生代停留时间窗口”策略后，短时间内被大量加载的页，并不会立刻插入新生代头部，而是优先淘汰那些，短期内仅仅访问了一次的页。

### 事务

**原子性A**：事务被视为不可分割的最小单元，事务的所有操作要么全部成功，要么全部失败回滚。

**一致性C**：数据库在事务执行前后都保持一致性状态，在一致性状态下，所有事务对一个数据的读取结果都是相同的。

**隔离性I**：一个事务所做的修改在最终提交以前，对其他事务是不可见的。

**持久性D**：一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢。  

![image-20210408193120394](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210408193120394.png)

* 事务的原子性是通过 undo log 来实现的

  * undo log有两个作用：**提供回滚和多版本控制(MVCC)。**

  * 在数据修改的时候，不仅记录了redo，还记录了相对应的undo，undo log主要记录的是数据的逻辑变化，为了在发生错误时回滚之前的操作，需要将之前的操作都记录下来，然后在发生错误时才可以回滚。

    undo日志，只将数据库逻辑地恢复到原来的样子，在回滚的时候，它实际上是做的相反的工作，比如一条INSERT ，对应一条 DELETE，对于每个UPDATE,对应一条相反的 UPDATE,将修改前的行放回去。undo日志用于事务的回滚操作进而保障了事务的原子性。

    实现原子性的关键，是当事务回滚时能够撤销所有已经成功执行的sql语句。 InnoDB 实现回滚，靠的是undo log ：当事务对数据库进行修改时，InnoDB 会生成对应的undo log 如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。

  * innodb中，undo log 分为：insert/update undo log

* 事务的持久性是通过 redo log 来实现的

  * Redo log可以简单分为以下两个部分：

    一是内存中的**重做日志缓冲** (redo log buffer), 是易失的，保存在内存
    二是**重做日志文件** (redo log file)，是持久的，保存在磁盘中

  * Redo的时机：

    在数据页修改完成之后，在脏页刷出磁盘之前，写入redo日志。注意的是**先修改数据，后写日志**
    redo日志比数据页先写回磁盘
    聚集索引、二级索引、undo页面的修改，均需要记录Redo日志。

  * ![image-20210408200439943](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210408200439943.png)

  * 当事务提交时，先将 redo log buffer 写入到 redo log file 进行持久化，待事务的commit操作完成时才算完成。这种做法也被称为 **Write-Ahead Log(预先日志持久化)**，在持久化一个数据页之前，先将内存中相应的日志页持久化。
  * 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（redo log buffer）里面，并更新内存（buffer pool），这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候（如系统空闲时），将这个操作记录更新到磁盘里面（刷脏页）。
  *  **redo log 的写入拆成了两个步骤：prepare 和 commit，这就是两阶段提交（2PC）。**MySQL 使用两阶段提交主要解决 binlog 和 redo log 的数据一致性的问题。
  * **binlog**：
    * ① redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
      ② **redo log 是物理日志**，记录的是“**在某个数据页上做了什么修改**”；binlog 是逻辑日志，**记录的是这个语句的原始逻辑**，比如“给 ID=2 这一行的 c 字段加 1 ”。
      ③ redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“**追加写**”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

* 事务的隔离性是通过 (读写锁+MVCC)来实现的

* 事务一致性是通过原子性，持久性，隔离性来实现的

 ![image-20210313215559514](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313215522432.png)

### 隔离级别

![image-20210313215522432](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313215559514.png)

**未提交读（READ UNCOMMITTED）**事务中的修改，即使没有提交，对其他事务也是可见的。

**提交读（READ COMMITTED）**一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其他事务是不可见的。

**可重复读（REPEATABLE READ）**保证在同一个事务中多次读取同样数据的结果是一样的。

**可串行化（SERIALIZABLE）**强制事务串行执行。需要加锁实现，而其它隔离级别通常不需要。

### InnoDB锁机制

**共享锁（S Lock）**允许事务读一行数据

**排他锁（X Lock）**允许事务删除或者更新一行数据

**意向共享锁（IS Lock）**事务想要获得一张表中某几行的共享锁

**意向排他锁**：事务想要获得一张表中某几行的排他锁

* 行锁表锁
  
  * InnoDB的行锁是针对索引加的锁，不是针对记录加的锁 ,并且该索引不能失效，否则会从行锁升级为表锁 。
    
    所以建表的时候 ，结合你的业务，如果有更新的操作，切记要对操作的字段建立索引，不然并发下这个问题就非常明显了
  
* FOR UPDATE操作对表是加行独占锁，只影响SELECT选中的行。
  
* MySQL InnoDB支持三种行锁定方式：

  l  行锁（Record Lock）:锁直接加在索引记录上面，锁住的是key。

  l  间隙锁（Gap Lock）:锁定索引记录间隙，确保索引记录的间隙不变。间隙锁是针对事务隔离级别为可重复读或以上级别而已的。

  l  后码锁 Next-Key Lock ：行锁和间隙锁组合起来就叫Next-Key Lock。

  * 默认情况下，InnoDB工作在可重复读隔离级别下，并且会以Next-Key Lock的方式**对数据行进行加锁**，这样可以有效防止幻读的发生。Next-Key Lock是行锁和间隙锁的组合，当InnoDB扫描索引记录的时候，会首先对索引记录加上行锁（Record Lock），再对索引记录两边的间隙加上间隙锁（Gap Lock）。加上间隙锁之后，其他事务就不能在这个间隙修改或者插入记录。

  * innodb对于行的查询（rr级别的当前读）使用next-key lock
  * Next-locking keying为了解决Phantom Problem幻读问题
  * 当查询的索引含有唯一属性时，将next-key lock降级为record key
  * Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生
  * 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1

* gap log间隙锁

  * innodb间隙锁就是不仅仅锁住所需要的行（如果锁住的这行不存在）还会锁住一个范围的行，这个范围依据锁住的这行而定。上下刚好是两个相邻索引叶节点的范围。包含下范围，不包含上范围。

  * Gap Lock在InnoDB的唯一作用就是防止其他事务的插入操作，以此防止幻读的发生。

  * 间隙锁（Gap Lock）一般是针对非唯一索引而言的

  * 假如要更新v1=7的数据行，那么此时会在索引idx_v1对应的值，也就是v1的值上加间隙锁，锁定的区间是（5,7）和（7,9）。同时找到v1=7的数据行的**主键索引和非唯一索引**，对key加上锁。

  * 开启和关闭

    *  A. 将事务隔离级别设置为**RC** 

      B. 将参数`innodb_locks_unsafe_for_binlog`设置为1

      C. 确保**where索引唯一** ，从而避让 gap lock

* Next-key locking

  * InnoDB 存储引擎默认隔离级别为可重复读（Repeatable Read），该隔离级别下加行锁采用的是 next-key locking 策略。
  * 记录锁和间隙锁的结合，对于InnoDB中，更新非唯一索引对应的记录（在这里来说是更新v1字段的值），会加上Next-Key Lock。如果更新记录为空，就不能加记录锁，只能加间隙锁。
  * InnoDB支持行锁（锁定字段含有索引的情况下，否则走表锁），但锁定方式并非简单的锁定指定行上的索引，而是分为3种锁定算法：
    1）记录锁（Record Locks）：锁定指定行的索引项
    2）Gap Locks：锁定某一个范围内的索引，但不包括记录本身
    3）临键锁（Next-Key Locks）：锁定一个范围内的索引，并且锁定记录本身   Next-Key Locks = Record Locks + Gap Locks。如果把事务的隔离级别降级为RC，临键锁则也会失效。
  * 使用for update时，若使用了索引则是行锁，否则是表锁
  * innodb下的记录锁(也叫行锁)，间隙锁，next-key锁统统属于排他锁。

### MVCC

多版本并发控制（Multi-Version Concurrency Control, MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现**提交读**和**可重复读**这两种隔离级别。而**未提交读隔离级别总是读取最新的数据行**，无需使用 MVCC。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。

**版本号**

- 系统版本号：是一个递增的数字（全局），每开始一个新的事务，系统版本号就会自动递增。
- 事务版本号：事务开始时的系统版本号。

**隐藏的列**

MVCC 在每行记录后面都保存着**两个隐藏的列**，用来存储两个版本号：

- 创建版本号：指示创建一个数据行的快照时的**系统版本号**；
- 删除版本号：如果该快照的删除版本号大于当前**事务版本号**表示该快照有效，否则表示该快照已经被删除了。

**Undo 日志**

MVCC 使用到的快照存储在 Undo 日志中，该日志通过**回滚指针**把一个**数据行（Record）的所有快照连接起来。**![image-20210313220605753](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313203225408.png)

* 在`读提交`隔离级别下，这个视图是在**每个SQL语句开始执行的时候创建**的，在这个隔离级别下，**事务在每次查询开始时都会生成一个独立的ReadView**

![image-20210314164118424](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210314164212504.png)

* 可重复读，在第一次读取数据时生成一个**ReadView**，对于使用`REPEATABLE READ`隔离级别的事务来说，**只会在第一次执行查询语句**时生成一个`ReadView`，之后的查询就不会重复生成了，所以**一个事务的查询结果每次都是一样的。**![image-20210314164212504](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210314164118424.png)

* 总结：在使用`读已提交（READ COMMITTD）、可重复读（REPEATABLE READ）`这两种隔离级别的事务在执行普通的SELECT操作时**访问记录的版本链**的过程，这样子可以使不同事务的读-写、写-读操作并发执行，从而提升系统性能。

  这两个隔离级别的一个很大不同就是：`生成ReadView的时机不同`，READ COMMITTD在**每一次进行普通SELECT操作前都会生成一个ReadView**，而REPEATABLE READ**只在第一次进行普通SELECT操作前生成一个ReadView**，数据的可重复读其实就是**ReadView的重复使用**。

* 幻读

  * 幻读，并不是说两次读取获取的结果集不同，幻读侧重的方面是某一次的 select 操作得到的结果所表征的数据状态无法支撑后续的业务操作。更为具体一些：select 某记录是否存在，不存在，准备插入此记录，但**执行 insert 时发现此记录已存在**，无法插入，再去查还是与之前结果一样没有该记录（MVCC下的RR可重复读），此时就发生了幻读。

  * `RR` 级别作为 `mysql` 事务默认隔离级别，是事务安全与性能的折中，可能也符合二八定律（20%的事务存在幻读的可能，80%的事务没有幻读的风险），我们在正确认识幻读后，便可以根据场景灵活的防止幻读的发生。

    `SERIALIZABLE` 级别则是悲观的认为幻读时刻都会发生，故会自动的隐式的对事务所需资源加排它锁，其他事务访问此资源会被阻塞等待，故事务是安全的，但需要认真考虑性能。

  * RR可重复读下防止幻读：

    * RR级别下只要对 SELECT 操作也手动加行（X）锁即可类似 SERIALIZABLE 级别（它会对 SELECT 隐式加锁）：

      ```
      # 这里需要用 X锁， 用 LOCK IN SHARE MODE 拿到 S锁 后我们没办法做 写操作
      SELECT `id` FROM `users` WHERE `id` = 1 FOR UPDATE;
      ```
      
    * Next-key locking解决幻读（当前读）问题


### MVCC+undo log实现可重复读

当开始一个事务时，**该事务的版本号肯定大于当前所有数据行快照的创建版本号**，理解这一点很关键。数据行快照的创建版本号是创建数据行快照时的系统版本号，系统版本号随着创建事务而递增

因此新创建一个事务时，这个事务的系统版本号比之前的系统版本号都大，也就是比所有数据行快照的创建版本号都大。

**SELECT**

多个事务必须**读取到同一个数据行的快照**，并且这个快照是**距离现在最近的一个有效快照**。但是也有例外，如果有一个事务**正在修改该数据行**，那么它可以读取事务本身所做的修改，而不用和其它事务的读取结果一致。

把没有对一个数据行做修改的事务称为 T，T 所要读取的数据行快照的创建版本号**必须小于等于 T 的版本号**。因为如果大于 T 的版本号，那么表示该数据行快照是其它事务的最新修改，因此不能去读取它。

除此之外，T 所要读取的数据行快照的**删除版本号必须是未定义或者大于 T 的版本号**，因为如果小于等于 T 的版本号，那么表示该数据行快照是已经被删除的，不应该去读取它。

**INSERT**

将当前系统版本号作为数据行快照的创建版本号。

**DELETE**

将当前系统版本号作为数据行快照的删除版本号。

**UPDATE**

将当前系统版本号作为更新前的数据行快照的删除版本号，并将当前系统版本号作为更新后的数据行快照的创建版本号。可以理解为先执行 DELETE 后执行 INSERT。

### 快照读和当前读

在可重复读级别中，通过MVCC机制，虽然让数据变得可重复读，但我们读到的数据可能是**历史数据**，是不及时的数据，不是数据库当前的数据！这在一些对于数据的时效特别敏感的业务中，就很可能出问题。

对于这种读取历史数据的方式，我们叫它快照读 (snapshot read)，而读取数据库当前版本数据的方式，叫当前读 (current read)。很显然，在MVCC中：

**快照读**：MVCC 的 SELECT 操作是快照中的数据，不需要进行加锁操作。

**当前读**：MVCC 其它会对数据库进行修改的操作（INSERT、UPDATE、DELETE）需要进行加锁操作（防止出现幻读），从而读取最新的数据。可以看到 MVCC 并不是完全不用加锁，而只是**避免了 SELECT 的加锁操作。**

在进行 SELECT 操作时，可以强制指定进行加锁操作。以下第一个语句需要加 S 锁，第二个需要加 X 锁。

```
- select * from table where ? lock in share mode;
- select * from table where ? for update;
```

事务的隔离级别实际上都是定义的当前读的级别，MySQL为了减少锁处理（包括等待其它锁）的时间，提升并发能力，引入了快照读的概念，使得select不用加锁。而update、insert这些“当前读”的隔离性，就需要通过加锁来实现了。

### 锁算法

Record Lock：锁定一个记录上的索引，而不是记录本身。

如果表没有设置索引，InnoDB 会自动在主键上创建隐藏的聚簇索引，因此 Record Locks 依然可以使用。

Gap Lock：锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15。

```
SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE;
```

Next-Key Lock：它是 Record Locks 和 Gap Locks 的结合，**不仅锁定一个记录上的索引，也锁定索引之间的间隙**。例如一个索引包含以下值：10, 11, 13, and 20，那么就需要锁定以下区间：

```
(-∞, 10]  (10, 11]   (11, 13]    (13, 20]    (20, +∞)
```

> **在 InnoDB 存储引擎中，SELECT 操作的不可重复读问题通过 MVCC 得到了解决，而 UPDATE、DELETE 的不可重复读问题通过 Record Lock 解决，INSERT 的不可重复读问题是通过 Next-Key Lock（Record Lock + Gap Lock）解决的。**

### 分库分表数据切分

高性能数据库集群有两种方式：
1、读写分离，其本质是将访问压力分散到集群中的多个节点，但是没有分散存储压力。

* 读写分离的基本实现：
  - 数据库服务器搭建主从集群，一主一从、一主多从都可以。
  - 数据库主机负责读写操作，从机只负责读操作。
  - 数据库主机通过复制将数据同步到从机，每台数据库服务器都存储了所有的业务数据。
  - 业务服务器将写操作发给数据库主机，将读操作发给数据库从机。

2、分库分表，既可以分散访问压力，又可以分散存储压力。

* 垂直切分

![image-20210313222742012](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210314101635252.png)

* 水平切分

![image-20210313222804986](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313222742012.png)

* Sharding 策略
  - 哈希取模：hash(key)%N
  - 范围：可以是 ID 范围也可以是时间范围
  - 映射表：使用单独的一个数据库来存储映射关系

* Sharding问题：
  * **事务问题**：使用分布式事务来解决，比如 XA 接口

    **连接**：可以将原来的连接分解成多个单表查询，然后在用户程序中进行连接。

    **唯一性**

    - 使用全局唯一 ID （GUID）
    - 为每个分片指定一个 ID 范围
    - 分布式 ID 生成器（如 Twitter 的 Snowflake 算法）

### 复制

主从复制

* binlog 线程 ：负责将**主服务器上的数据更改写入二进制日志（Binary log）**中。

* I/O 线程 ：负责**从主服务器上读取 二进制日 志binlog**，并**写入从服务器的中继日志（Relay log）**。

* SQL 线程 ：负责**读取中继日志**，解析出主服务器已经执行的数据更改并**在从服务器中重放（Replay）**（Redis的主从复制也是类似）

读写分离

* 主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。
* 读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。
* 主从服务器负责各自的读和写，极大程度缓解了锁的争用；
* 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销；
* 增加冗余，提高可用性。

![image-20210313223141323](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313223141323.png)

### JSON

1. 在 where 条件中有通过 json 中的某个字段去过滤返回结果的需求
2. 查询 json 字段中的部分字段作为返回结果（减少内存占用）

* **JSON_CONTAINS(target, candidate [, path])**:如果在 json 字段 target 指定的位置 path，找到了目标值 condidate，返回 1，否则返回 0

  如果只是检查在指定的路径是否存在数据，使用JSON_CONTAINS_PATH()

* **JSON_CONTAINS_PATH(json_doc, one_or_all, path[, path] ...)**:   如果在指定的路径存在数据返回 1，否则返回 0

### 范式

函数依赖：记 A->B 表示 A 函数决定 B，也可以说 B 函数依赖于 A。

第一范式 (1NF)：属性不可分。

第二范式 (2NF)：每个非主属性完全函数依赖于唯一一个键码。可以通过分解来满足。

第三范式 (3NF)：非主属性不传递函数依赖于键码。 （可以去掉，增加冗余字段提高查询效率）

ER图：有三个组成部分：实体、属性、联系。用来进行关系型数据库系统的概念设计。

### 磁盘IO和索引

* 三类基本的缓存管理和优化问题：
  * 预取(`prefetching`)算法，从慢速存储中加载数据到缓存; 即预读机制，磁盘的机械臂+旋转盘片的数据定位与读取方式，所以擅长顺序IO
  * 替换(`replacement`)算法，从缓存中丢弃无用数据;
  * 写回(`writeback`)算法，把脏数据从缓存中保存到慢速存储。

* 文件预读机制： 当一次IO时，不光把当前磁盘地址的数据，而是把**相邻的数据**也都读取到内存缓冲区内，因为当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。

  * 顺序性检测：

    * 为了保证预读命中率，Linux只对顺序读(`sequential read`)进行预读。内核通过验证如下两个条件来判定一个`read()`是否顺序读：
      - 这是文件被打开后的第一次读，并且读的是文件首部;
      - 当前的读请求与前一(记录的)读请求在文件内的位置是连续的。

  * 预读大小：

    * 当确定了要进行顺序预读(sequential readahead)时，就需要决定合适的预读大小。预读粒度太小的话，达不到应有的性能提升效果;预读太多，又有可能载入太多程序不需要的页面，造成资源浪费。为此，Linux采用了一个快速的窗口扩张过程：

      首次预读：`readahead_size = read_size * 2;  // or *4`
      预读窗口的初始值是**读大小的二到四倍**。这意味着在您的程序中使用较大的读粒度 (比如32KB) 可以稍稍提升I/O效率。

      后续预读：`readahead_size *= 2;`  后续的预读窗口将逐次倍增，直到达到系统设定的最大预读大小，其缺省值是`128KB`。

* 每一次IO读取的数据我们称之为一页(page)，具体一页有多大数据跟操作系统有关，一般为**4k或8k**，也就是我们读取**一页内**的数据时候，实际上才发生了**一次IO。**

* `MySQL`中常用的索引在物理上分两类，**B-树索引和哈希索引。**

* B-Tree索引：这是一个3叉（只是举例，真实会有很多叉）的BTree结构图，每一个方框块我们称之为一个磁盘块或者叫做一个block块，这是操作系统一次IO往内存中读的内容，一个块对应四个扇区，紫色代表的是磁盘块中的数据key，黄色代表的是数据data，蓝色代表的是指针p，指向下一个磁盘块的位置。![image-20210314101248316](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210314101248316.png)

* B+Tree索引：`B+Tree`是在`B-Tree`基础上的一种优化，使其更适合实现外存储索引结构。在B+Tree中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+Tree的高度。每个Leaf Node是三部分组成的，即前驱指针p_prev，数据data以及后继指针p_next，同时数据data是有序的，默认是升序ASC，分布在B+tree右边的键值总是大于左边的，同时从root到每个Leaf的距离是相等的，也就是访问任何一个Leaf Node需要的IO是一样的，即索引树的高度Level + 1次IO操作。![image-20210314101635252](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210314172355805.png)

* B+Tree非叶子节点只存储键值信息， 数据记录都存放在叶子节点中， 将B-Tree优化，由于B+Tree的非叶子节点只存储键值信息，所以B+Tree的高度可以被压缩到特别的低。

* 聚集索引的B+Tree中的叶子节点存放的是整张表的行记录数据，辅助索引与聚集索引的区别在于辅助索引的叶子节点并不包含行记录的全部数据，而是存储相应行数据的聚集索引键，即主键。

* 索引只是提高效率的一个因素，因此在建立索引的时候应该遵循以下原则：
  - 在经常需要**搜索的列**上建立索引，可以加快搜索的速度。
  - 在作为**主键的列**上创建索引，强制该列的唯一性，并组织表中数据的排列结构。
  - 在经常使用**表连接的列**上创建索引，这些列主要是一些外键，可以加快表连接的速度。
  - 在经常需要根据范围进行**搜索的列**上创建索引，因为索引已经排序，所以其指定的范围是连续的。
  - 在经常需要**排序的列**上创建索引，因为索引已经排序，所以查询时可以利用索引的排序，加快排序查询。
  - 在经常使用 **WHERE 子句的列**上创建索引，加快条件的判断速度。
  
* 虽然索引可以加快查询速度，提高 MySQL 的处理性能，但是过多地使用索引也会造成以下**弊端**：
  - 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。
  - 除了数据表占数据空间之外，每一个索引还要占一定的物理空间。如果要建立聚簇索引，那么需要的空间就会更大。
  - 当对表中的数据进行增加、删除和修改的时候，索引也要动态地维护，这样就降低了数据的维护速度。
  
* 叶子节点是一个递增的数组，可以用二分法搜索

### int和byte

int占4个字节，即表示int类型的存储大小为4个字节。
如果转成十进制来说就是“-2147483648 ~2147483647”
即：int只能存放这么大的数字。。。超出范围则溢出。。。

再来说byte
byte最大能够存放 -128～127 的数值。

那为什么是 -128～127 
这个跟字节编码有关 
首先知道byte是一个字节保存的，有8个位，也就是8个0、1。 
**8个位的第一个位是符号位**， 
也就是说0000 0001代表的是数字1 
1000 0000代表的就是-1 
所以正数最大位0111 1111，也就是数字127 
负数最大为1111 1111，也就是数字-128 

### 索引设计

* MySQL中的索引可以看成一张小表，占用磁盘空间，创建索引的过程其实就是按照索引列排序的过程，先在sort_buffer_size进行排序，如果排序的数据量大，sort_buffer_size容量不下，就需要通过临时文件来排序，最重要的是通过索引可以避免排序操作（distinct，group by，order by）。

* 对于OLAP(联机分析处理)的业务场景，需要扫描返回大量数据，这时候全表扫描的顺序IO效率更高。

* 全表扫描是顺序IO，索引扫描是随机IO，MySQL对此做了优化，增加了change buffer特性来提高IO性能。

* 分页查询优化：利用辅助索引的覆盖扫描进行优化，先获取id，这一步就是索引覆盖扫描，不需要回表，然后通过id跟原表trade_info进行关联

  ```
  select * from trade_info where status = 0 and create_time >= '2020-10-01 00:00:00' and create_time <= '2020-10-07 23:59:59' order by id desc limit 102120, 20;
  // 优化为：
  select * from trade_info a ,
  (select  id from trade_info where status = 0 and create_time >= '2020-10-01 00:00:00' and create_time <= '2020-10-07 23:59:59' order by id desc limit 102120, 20) as b   //这一步走的是索引覆盖扫描，不需要回表
   where a.id = b.id;
  ```

* 前缀索引：-- 创建前缀索引，前缀长度为30  `create index idx_nickname_part on users(nickname(30));`

* 复合索引：**唯一值多选择性好的列作为复合索引的前导列。**但只适合于等值条件过滤，不适合有范围条件过滤的情况，如：`elect * from trade_info where status = 1 and create_time >= '2020-10-01 00:00:00' and create_time <= '2020-10-07 23:59:59';`
  * 原则1：将范围查询的列放在复合索引的最后面，例如idx_status_create_time。
  * 原则2：列过滤的频繁越高，选择性越好，应该作为复合索引的前导列，适用于等值查找，例如idx_user_id_status。

* 跳跃索引：**适合复合索引前导列唯一值少，后导列唯一值多的情况，如果前导列唯一值变多了，则MySQL CBO不会选择索引跳跃扫描，取决于索引列的数据分表情况。**
* **最左前缀匹配原则**。这是非常重要、非常重要、非常重要（重要的事情说三遍）的原则，MySQL会一直向右匹配直到遇到范围查询 （>,<,BETWEEN,LIKE）就停止匹配。
* 尽量选择**区分度高的列作为索引**，区分度的公式是 COUNT(DISTINCT col)/COUNT(*)。表示字段不重复的比率，比率越大我们扫描的记录数就越少。
* **索引列不能参与计算，尽量保持列“干净”**。比如， FROM_UNIXTIME(create_time)='2016-06-06' 就不能使用索引，原因很简单，**B+树中存储的都是数据表中的字段值**，但是进行检索时，需要把所有元素都应用函数才能比较，显然这样的代价太大。所以语句要写成 ：create_time=UNIX_TIMESTAMP('2016-06-06')。
* 尽可能的**扩展索引**，不要新建立索引。比如表中已经有了a的索引，现在要加（a,b）的索引，那么只需要修改原来的索引即可。
* 单个多列组合索引和多个单列索引的检索查询效果不同，因为在执行SQL时，***MySQL只能使用一个索引**，会从多个单列索引中选择一个限制最为严格的索引*(经指正，在MySQL5.0以后的版本中，有“合并索引”的策略，翻看了《高性能MySQL 第三版》，书作者认为：**还是应该建立起比较好的索引，而不应该依赖于“合并索引”这么一个策略**)。
* “合并索引”策略简单来讲，就是使用多个单列索引，然后将这些结果用“union或者and”来合并起来

### 主键

* 自增ID达到上限用完了之后，分为两种情况：
  1. 如果设置了主键，那么将会报错主键冲突。
  2. 如果没有设置主键，数据库则会帮我们自动生成一个全局的row_id，新数据会覆盖老数据

* 解决方案：

  表尽可能都要设置主键，主键尽量使用bigint类型，21亿的上限还是有可能达到的

### DDL

* CREATE，ALTER，DROP，RENAME，TRUNCATE。这些操作都是隐式提交且原子性，要么成功，要么失败，在MySQL 8.0之前DDL操作是不记录日志的。
* ![image-20210314140712320](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210314144845733.png)
* MetaData元数据：描述的对象的结构信息，一般都是静态化，只有表上发生了DDL操作才会实时更新。
* MySQL利用MetaData Lock来管理对象的访问，保证数据的一致性，对于一些核心业务表，表上DML操作比较频繁，这个时候添加字段可能会触发MetaData Lock。
* **DDL执行方式**:  ALTER TABLE的选项很多，跟性能相关的选项主要有ALGORITHM和LOCK。
  * copy：MySQL早期的变更方式，需要创建修改后的临时表，然后按数据行拷贝原表数据到临时表，做rename重命名来完成创建，在此期间不允许并发DML操作，原表是可读的，不可写，同时需要额外一倍的磁盘空间。
  * INPLACE：直接在原表上进行修改，不需创建临时表拷贝数据及重命名，原表会持有Exclusive Metadata  Lock，通常是允许并发DML操作。
  * INSTANT：MySQL 5.8开始支持，只修改数据字典中的元数据，表数据不受影响，执行期间没有Exclusive Metadata  Lock，允许并发的DML操作。

* LOCK：无锁、共享锁、排它锁

* MySQL 8.0推出了INSTANT方式，真正的只修改MetaData，不影响表数据，所以它的执行效率跟表大小几乎没有关系。既解决主从同步，又解决rename数仓不同步的问题

* **监控DDL执行进度**：在MySQL 8.0可以通过开启performance_schema，打开events_stages_current事件进行监控。

### log

* binlog：**数据库的变更，搜索引擎的数据也需要变更**。搜索引擎监听`binlog`的变更，如果`binlog`有变更了，那我们就需要将变更写到对应的数据源。即记录了数据库表结构和表数据变更，不会记录select。存储着每条变更的`SQL`语句

  * 作用：**复制和恢复数据**，实现主从复制
  * 主从服务器需要保持数据的一致性，通过`binlog`来同步数据。
  * 如果整个数据库的数据都被删除了，`binlog`存储着所有的数据变更情况，那么可以通过`binlog`来对数据进行恢复。

* redo log：

  ![image-20210314144845733](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210314140712320.png)

  `redo log`的存在为了：当我们修改的时候，写完内存了，但数据还没真正写到磁盘的时候。此时我们的数据库挂了，我们可以根据`redo log`来对数据进行恢复。因为`redo log`是顺序IO，所以**写入的速度很快**，并且`redo log`记载的是物理变化（xxxx页做了xxx修改），文件的体积很小，**恢复速度很快**。

  `redo log` 记录的是数据的**物理变化**，`binlog` 记录的是数据的**逻辑变化**。`binlog`记载的是`update/delete/insert`这样的SQL语句，而`redo log`记载的是物理修改的内容（xxxx页修改了xxx）。

  `redo log`是MySQL的InnoDB引擎所产生的。`binlog`无论MySQL用什么引擎，都会有的。

  持久性就是靠`redo log`来实现的

* undo log：

  * 作用：回滚和多版本控制(MVCC)，实现事务的原子性
  * 主要存储的也是逻辑日志，比如我们要`insert`一条数据了，那`undo log`会记录的一条对应的`delete`日志。我们要`update`一条记录时，它会记录一条对应**相反**的update记录。
  * `undo log`存储着修改之前的数据，相当于一个**前版本**，MVCC实现的是读写不阻塞，读的时候只要返回前一个版本的数据就行了。

### Text类型

由于MySQL是单进程多线程模型，一个SQL语句无法利用多个cpu core去执行，这也就决定了MySQL比较适合OLTP（特点：大量用户访问、逻辑读，索引扫描，返回少量数据，SQL简单）业务系统，同时要针对MySQL去制定一些建模规范和开发规范，尽量避免使用Text类型，它不但消耗大量的网络和IO带宽，同时在该表上的DML操作都会变得很慢。

另外建议将复杂的统计分析类的SQL，建议迁移到实时数仓OLAP中，例如目前使用比较多的clickhouse，里云的ADB，AWS的Redshift都可以，做到OLTP和OLAP类业务SQL分离，保证业务系统的稳定性。

* 使用es存储：在MySQL中，一般log表会存储text类型保存request或response类的数据，用于接口调用失败时去手动排查问题，使用频繁的很低。可以考虑写入本地log file，通过filebeat抽取到es中，按天索引，根据数据保留策略进行清理。

* 使用对象存储：有些业务场景表用到TEXT，BLOB类型，存储的一些图片信息，比如商品的图片，更新频率比较低，可以考虑使用对象存储，例如阿里云的OSS，AWS的S3都可以，能够方便且高效的实现这类需求。

### InnoDB表空间

* **系统表空间：** 主要存储MySQL内部的数据字典数据，如information_schema下的数据。

  **用户表空间：** 当开启innodb_file_per_table=1时，数据表从系统表空间独立出来存储在以table_name.ibd命令的数据文件中，结构信息存储在table_name.frm文件中。

  **Undo表空间：** 存储Undo信息，如快照一致读和flashback都是利用undo信息。

* delete物理删除既不能释放磁盘空间，而且会产生大量的碎片，导致索引频繁分裂，影响SQL执行计划的稳定性；
* 在业务代码层面，应该做逻辑标记删除，避免物理删除；为了实现数据归档需求，可以用采用MySQL分区表特性来实现，都是DDL操作，没有碎片产生。
* 在MySQL数据库建模规范中有4个公共字段: `id、is_deleted、create_time、update_time`，基本上每个表必须有的，同时在create_time列要创建索引，有两方面的好处：
  1. 一些查询业务场景都会有一个默认的时间段，比如7天或者一个月，都是通过create_time去过滤，走索引扫描更快。
  2. 一些核心的业务表需要以T +1的方式抽取数据仓库中，比如每天晚上00:30抽取前一天的数据，都是通过create_time过滤的。

* 采用Clickhouse，对有生命周期的数据表可以使用Clickhouse存储，利用其TTL特性实现无效数据自动清理。

### SQL优化

* **UDF用户自定义函数**：SQL语句的select后面使用了自定义函数UDF，SQL返回多少行，那么UDF函数就会被调用多少次，这是非常影响性能的：`select id, payment_id, order_sn, getOrderNo(order_sn) from payment_transaction `

* text类型；如果select出现text类型的字段，就会消耗大量的网络和IO带宽，由于返回的内容过大超过max_allowed_packet设置会导致程序报错

* **group_concat**：gorup_concat是一个字符串聚合函数，会影响SQL的响应时间，如果返回的值过大超过了max_allowed_packet设置会导致程序报错。

* 内联子查询：在select后面有子查询的情况称为内联子查询，SQL返回多少行，子查询就需要执行过多少次，严重影响SQL性能。`select id,(select rule_name from member_rule limit 1) as rule_name, member_id, member_type, member_name, status  from member_info`

* **表的链接方式**：在MySQL中不建议使用Left Join，即使ON过滤条件列索引，一些情况也不会走索引，导致大量的数据行被扫描，SQL性能变得很差，同时要清楚ON和Where的区别。

* **子查询**：由于MySQL的基于成本的优化器CBO对子查询的处理能力比较弱，不建议使用子查询，可以改写成Inner Join。

  ```
  select b.member_id,b.member_type, a.create_time,a.device_model from member_operation_log a inner join (select member_id,member_type from member_base_info where `status` = 1
  and create_time between '2020-10-01 00:00:00' and '2020-10-30 00:00:00') as b on a.member_id = b.member_id;
  ```

* **索引列被运算**：当一个字段被索引，同时出现where条件后面，是不能进行任何运算，会导致索引失效。

* 类型转换：对于Int类型的字段，传varchar类型的值是可以走索引，MySQL内部自动做了隐式类型转换；相反对于varchar类型字段传入Int值是无法走索引的，应该做到对应的字段类型传对应的值总是对的。

* **列字符集**：从MySQL 5.6开始建议所有对象字符集应该使用用utf8mb4，包括MySQL实例字符集，数据库字符集，表字符集，列字符集。避免在关联查询Join时字段字符集不匹配导致索引失效，同时目前只有utf8mb4支持emoji表情存储。
* **前缀索引**：group/order by后面的列有索引，索引可以消除排序带来的CPU开销，如果是group by条件是前缀索引，是不能消除排序的。

* **函数运算**：假设需要统计某月每天的新增用户量，虽然可以走create_time的索引，但是不能消除排序，可以考虑冗余一个字段stats_date date类型来解决这种问题。
* **limit m,n要慎重**:  对于limit m, n分页查询，越往后面翻页即m越大的情况下SQL的耗时会越来越长，对于这种应该先取出主键id，然后通过主键id跟原表进行Join关联查询。

* 字段顺序：排序字段顺序，asc/desc升降要跟索引保持一致，充分利用索引的有序性来消除排序带来的CPU开销。

* 时间列索引：对于默认字段created_at(create_time)、updated_at(update_time)这种默认就应该创建索引，这一般来说是默认的规则。
* 复合索引顺序：MySQL遵循的是索引最左匹配原则，对于复合索引，从左到右依次扫描索引列，到遇到第一个范围查询（>=, >,<, <=, between ….. and ….）就停止扫描
* 索引属性；索引基数指的是被索引的列唯一值的个数，唯一值越多接近表的count(*)说明索引的选择率越高，通过索引扫描的行数就越少，性能就越高，例如主键id的选择率是100%，在MySQL中尽量所有的update都使用主键id去更新，因为id是聚集索引存储着整行数据，不需要回表，性能是最高的。即先查出主键再更新数据
* comment属性：字段的备注要能明确该字段的作用，尤其是某些表示状态的字段，要显式的写出该字段所有可能的状态数值以及该数值的含义。
* default属性：在创建表的时候，建议每个字段尽量都有默认值，禁止DEFAULT NULL，而是对字段类型填充响应的默认值。
* not null属性：根据业务含义，尽量将字段都添加上NOT NULL DEFAULT VALUE属性，如果列值存储了大量的NULL，会影响索引的稳定性。
* AUTO_INCREMENT属性:  在InnoDB内部是通过一个系统全局变量dict_sys.row_id来计数，如果id的值达到了最大值，下一个值就从0开始继续循环递增，在代码中禁止指定主键id值插入。
* 表&列名关键字:  在数据库设计建模阶段，对表名及字段名设置要合理，不能使用MySQL的关键字，如desc, order, status, group等。同时建议设置lower_case_table_names = 1表名不区分大小写。

### 安全

* 撞库:”撞库”是黑客通过收集互联网已泄露的用户和密码信息，生成对应的字典表，尝试批量登陆其他网站后，得到一系列可以登录的用户。

* 脱库：指从数据库中导出数据。它被用来指网站遭到入侵后，黑客窃取其数据库。
* 洗库：“洗库”，属于黑客入侵的一种，就是黑客入侵网站，通过技术手段将有价值的用户数据归纳分析，售卖变现。

### 设计数据库

* 根据业务模块的需求去分析，抽取成CDM中的E-R图，转换成LDM，经过数据库选型及生成PDM，最终生成数据库表，然后才能开始coding，测试、发布上线以及版本迭代，为了保证线上业务的安全稳定高效，就需要对数据库进行精细化管理和维护。

* select count(*):  
  * 针对无 where_clause 的 **COUNT(\*)**，MySQL 是有优化的，优化器会选择成本最小的辅助索引查询计数，其实反而性能最高
  * *不管是 COUNT(1)，还是 COUNT(*)，MySQL 都会用**成本最小**的辅助索引查询方式来计数，也就是使用 COUNT(*) 由于 MySQL 的优化已经保证了它的查询性能是最好的！随带提一句，COUNT(*)是 SQL92 定义的标准统计行数的语法，并且效率高，所以请直接使用COUNT(*)查询表的行数！

* 数据库连接池大小：连接数 = ((核心数 * 2) + 有效磁盘数)  。4核i7数据库服务器的连接池大小应该为((4 * 2) + 1) = 9

### 分库分表

* 垂直分表：常规的方案是冷热分离（将使用频率高字段放到一张表里，剩下使用频繁低的字段放到另一张表里）。

* 水平分表：按照表中的记录进行分片。 单表不建议超过 500w，1亿数据分20个子表就够了。
* 按月分表：根据业务的特性，可以按月创建表
* 分区表：保证 SQL 正确的路由，执行并返回结果。常规的 hash 也是基于分区个数取模（%）运算的

* 按业务分库：例如将电商中，将数据库分为订单数据库、价格数据库、库存数据库
* 按表分库：分库其实是拆分 RDS 实例，是将拆分后的子表存储在不同的 RDS 实例中。扩容一台同配置的RDS实例，将订单表 orders 拆分20个子表，每个 RDS 实例10个。查询的时候要先通过分区键 user_id 定位是哪个 RDS 实例，再定位到具体的子表，然后做 DML操作

* 分布式数据库：代码改造（数据库中间件mycat，sharding-sphere）和分布式数据库（实际业务中使用比较多的有 PingCAP TiDB，阿里云 DRDS）
* 分库分表主要有两种方案：代码改造（数据库中间件mycat，sharding-sphere）和分布式数据库（实际业务中使用比较多的有 PingCAP TiDB，阿里云 DRDS），可以优先使用分布式数据库方案，虽然成本会有所增加，但对应用程序没有侵入性，同时也可以比较好的支撑业务增长和系统快速迭代

### 主从复制

- MySql主库在事务提交时会把数据变更作为事件记录在二进制日志Binlog中；
- 主库推送二进制日志文件Binlog中的事件到从库的中继日志Relay Log中，之后从库根据中继日志重做数据变更操作，通过逻辑复制来达到主库和从库的数据一致性；
- MySql通过三个线程来完成主从库间的数据复制，其中**Binlog Dump**线程跑在主库上，**I/O线程和SQL线程**跑着从库上；
- 当在从库上启动复制时，首先创建I/O线程连接主库，主库随后创建Binlog Dump线程读取数据库事件并**发送给I/O线程**，I/O线程获取到事件数据后更新到从库的**中继日志Relay Log**中去，之后从库上的**SQL线程**读取中继日志Relay Log中更新的数据库事件并应用，如下图所示。

![image-20210314172355805](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210313222804986.png)



### MySQL数据页

**数据页之间**是以**双向链表的形式存储**，数据页里的**记录以单链表的形式连接**。

**全表扫描**的方式：即定位到第一个数据页，然后**从第一条记录开始遍历**，依次遍历完所有记录开始遍历第二个数据页，直到**全部遍历完**
数据在innodb是以**B+树(数据结构)**的形式存储，MySQL会默认给每张表创建**聚蔟索引**。

聚蔟索引的特点是**根据主键大小按升序创建**，在**B+树里面分层**，**底部叶子层**存储的是一张张的数据页(**数据页的记录包含所有数据，即所谓的索引即数据**)，

**非叶子层**存储的则是**目录结构(存储主键+数据页号)**，用于快速定位对应的数据页。所以，**所谓的全表扫描，就是扫描聚蔟索引的叶子层数据！**
因为innodb**默认创建聚簇索引**，所以当我们用主键来查找数据的时候就非常快了，能**利用到索引目录的结构非常迅速定位到目标**！

##### 数据页类型

  MySQL的数据页类型有十多种，包括：**索引页，Undo页，Inode页，系统页，BloB页**等，但我们最常用的就是**索引页**

##### 数据页头

  数据页的**header**即**数据页头**就是**记载数据页的相关信息**，可以说是数据页的**核心部分**(除了记录的真实数据)了，所以掌握了头部，就相当于掌握了数据页的**结构。**

![image-20201107170116020](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20201107170116020.png)

##### innodb数据页结构

页（Page）是 Innodb 存储引擎用于管理数据的**最小磁盘单位**。常见的页类型有**数据页、Undo 页、系统页、事务数据**页等，默认的**页大小为 16KB**，每个页中**至少存储有 2 条或以上的行记录Row**

下图是 Innodb **逻辑存储结构图**，从上往下依次为：**Tablespace**、**Segment**、**Extent**、**Page** 以及 **Row**。重点是 Page 和 Row 的数据结构。

![image-20201107165038832](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20201107165038832.png)

##### Page

![image-20201107170151912](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20201107170151912.png)

上图为 Page 数据结构

**File Header** 字段用于记录 **Page 的头信息**，其中比较重要的是 **FIL_PAGE_PREV** 和 **FIL_PAGE_NEXT** 字段，通过这两个字段，我们可以找到该页的**上一页和下一页**，实际上所有页通过两个字段可以形成一条**双向链表**。

**Page Header** 字段用于记录 **Page 的状态信息**。接下来的 **Infimum** 和 **Supremum** 是两个**伪行记录**，Infimum（**下确界**）记录**比该页中任何主键值都要小的值**，Supremum （**上确界**）记录**比该页中任何主键值都要大的值**，这个伪记录分别构成了**页中记录的边界**。

![image-20201107170311943](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20201107170311943.png)

**User Records** 中存放的是**实际的数据行记录Row**。

**Free Space** 中存放的是**空闲空间**，**被删除的行记录**会被记录成空闲空间。

**Page Directory** 记录着与**二叉查找相关的信息**。

**File Trailer** 存储用于**检测数据完整性的校验**的数据

##### Row

Innodb 存储引擎提供了两种格式的行记录：**Compact** 和 **Redundant**。

##### Compact 行记录

![image-20201203135627148](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20201203135627148.png)

**变长字段长度列表**：**逆序记录每一个列的长度**，如果列的长度**小于 255 字节**，则使用**一个字节**，**否则使用 2 个字节**。该字段的实际长度取决于**列数和每一列的长度**，因此是变长的。

**NULL 标志位**：一个字节，**表示该行是否有 NULL 值**

**记录头信息**：五个字节，其中 **next_record** 记录了**下一条记录的相对位置**，一个页中的所有记录使用这个字段**形成了一条单链表。**

**列数据部分**：除了**记录每一列对应的数据外**，还有**隐藏列**，它们分别是 **Transaction ID、Roll Pointer 以及 row_id（当没有指定主键）**。

**注意**：此处需要注意**固定长度 CHAR 数据类型和变长 VCHAR 数据类型**在 **Compact 记录下为 NULL 时不占用任何存储空间。**

##### Redundant 行记录

![image-20201203135639612](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20201203135639612.png)

字段长度偏移列表：与 Compact 中的**变长字段长度列表**相同的是它们**都是按照列的逆序顺序设置值**的，不同的是**字段长度偏移列表**记录的是**偏移量**，每一次都需要加上**上一次的偏移**，同时对于 **CHAR 的 NULL 值**，会直接**按照最大空间记录**，而对于 **VCHAR 的 NULL 值不占用任何存储空间**。

**注意**：此处需要注意 VCHAR 类型和 CHAR 类型在建表时传入的参数是**字符长度而不是字节长度**，实际的字节长度需要**跟编码方式相关联**，例如 UTF-8 一个中文字符需要 **3 字节**来表示，这样 CHAR(10) 以 UTF-8 来表示的话，它的字节长度在 **10 - 30** 之间。

##### 行溢出

我们知道数据页的大小是 **16KB**，Innodb 存储引擎保证了**每一页至少有两条记录**，如果一页当中的记录过大，会**截取前 768 个字节存入页中，其余的放入 BLOB Page。**

* explain原理-采集样本数据

  * 1、BERNOULLI(行级别伯努利采样)：它检查每一行，准确率高，但是性能差。

    2、SYSTEM(系统页级采样)：它检查每一数据页(一个数据页包含若干行)，性能高，但准确率差。









