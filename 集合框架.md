### Collections

![image-20210328122528199](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210328122528199.png)

* 基本操作

  ```
  int binarySearch(List list, Object key)//对List进行二分查找，返回索引，注意List必须是有序的
  int max(Collection coll)//根据元素的自然顺序，返回最大的元素。 类比int min(Collection coll)
  int max(Collection coll, Comparator c)//根据定制排序，返回最大元素，排序规则由Comparatator类控制。类比int min(Collection coll, Comparator c)
  void fill(List list, Object obj)//用指定的元素代替指定list中的所有元素。
  int frequency(Collection c, Object o)//统计元素出现次数
  int indexOfSubList(List list, List target)//统计target在list中第一次出现的索引，找不到则返回-1，类比int lastIndexOfSubList(List source, list target).
  boolean replaceAll(List list, Object oldVal, Object newVal), 用新元素替换旧元素
  ```

* 排序操作

  ```
  void reverse(List list)//反转
  void shuffle(List list)//随机排序
  void sort(List list)//按自然排序的升序排序
  void sort(List list, Comparator c)//定制排序，由Comparator控制排序逻辑
  void swap(List list, int i , int j)//交换两个索引位置的元素
  void rotate(List list, int distance)//旋转。当distance为正数时，将list后distance个元素整体移到前面。当distance为负数时，将 list的前distance个元素整体移到后面
  ```

* `Collections` 提供了多个`synchronizedXxx()`方法·，该方法可以将指定集合包装成线程同步的集合，从而解决多线程并发访问集合时的线程安全问题。我们知道 `HashSet`，`TreeSet`，`ArrayList`,`LinkedList`,`HashMap`,`TreeMap` 都是线程不安全的。`Collections` 提供了多个静态方法可以把他们包装成线程同步的集合。

###  排序

- `comparable` 接口实际上是出自`java.lang`包 它有一个 `compareTo(Object obj)`方法用来排序

- `comparator`接口实际上是出自 java.util 包它有一个`compare(Object obj1, Object obj2)`方法用来排序

  ```java
  // 定制排序的用法
  Collections.sort(arrayList, new Comparator<Integer>() {
      @Override
      public int compare(Integer o1, Integer o2) {
          // o1和o2反过来就是倒序排序
          return o2.compareTo(o1);
      }
  });
  
  public  class Person implements Comparable<Person> {
      @Override
      public int compareTo(Person o) {
          //  顺序排序
          if (this.age > o.getAge()) {
              return 1;
          }
          // 倒序排序
          if (this.age < o.getAge()) {
              return -1;
          }
        return 0;
      }
  }
  ```
  

### HashMap

* 与HashTable区别

  * **线程是否安全：** `HashMap` 是非线程安全的，`HashTable` 是线程安全的,因为 `HashTable` 内部的方法基本都经过`synchronized` 修饰。（如果你要保证线程安全的话就使用 `ConcurrentHashMap` 吧！）；
  * **效率：** 因为线程安全的问题，`HashMap` 要比 `HashTable` 效率高一点。另外，`HashTable` 基本被淘汰，不要在代码中使用它；
  * **对 Null key 和 Null value 的支持：** `HashMap` 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个；HashTable 不允许有 null 键和 null 值，否则会抛出 `NullPointerException`。
  * **初始容量大小和每次扩充容量大小的不同 ：** ① 创建时如果不指定容量初始值，`Hashtable` 默认的初始大小为 **11**，之后每次扩充，容量变为原来的 **2n+1**。`HashMap` 默认的初始化大小为 **16**。之后每次扩充，容量变为原来的 **2 倍**。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 `HashMap` 会将其扩充为 2 的幂次方大小（`HashMap` 中的`tableSizeFor()`方法保证）。也就是说 `HashMap` 总是使用 2 的幂作为哈希表的大小
  * **底层数据结构：** JDK1.8 以后的 `HashMap` 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。

* 与HashSet区别：

  * `HashSet` 底层就是基于 `HashMap` 实现的。（`HashSet` 的源码非常非常少，因为除了 `clone()`、`writeObject()`、`readObject()`是 `HashSet` 自己不得不实现之外，其他方法都是直接调用 `HashMap` 中的方法。

* 与TreeMap区别

  * `TreeMap` 和`HashMap` 都继承自`AbstractMap` ，但是需要注意的是`TreeMap`它还实现了`NavigableMap`接口和`SortedMap` 接口。

  * 实现 `NavigableMap` 接口让 `TreeMap` 有了**对集合内元素的搜索的能力**。

    实现`SortMap`接口让 `TreeMap` 有了**对集合中的元素根据键排序**的能力。默认是**按 key 的升序排序**，不过我们也**可以指定排序的比较器**。

    因此**相比于`HashMap`来说 `TreeMap` 主要多了对集合中的元素根据键排序的能力以及对集合内元素的搜索的能力。**

    ```
    TreeMap<Person, String> treeMap = new TreeMap<>((person1, person2) -> {
      int num = person1.getAge() - person2.getAge();
      return Integer.compare(num, 0);
    });
    ```

* 底层实现

  * 1.8之前：

    * JDK1.8 之前 `HashMap` 底层是 **数组和链表** 结合在一起使用也就是 **链表散列**。**HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) & hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。**


    * 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。1.7的hash方法性能差些，扰动了4次
    
    * 所谓 **“拉链法”** 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。
    * 1.8之前的头插法导致 HashMap多线程操作导致死循环问题
      * 主要原因在于并发下的 Rehash 会造成元素之间会形成一个循环链表。不过，jdk 1.8 后解决了这个问题，但是还是不建议在多线程下使用 HashMap,因为多线程下使用 HashMap 还是会存在其他问题比如数据丢失。并发环境下推荐使用 ConcurrentHashMap 。

  * 1.8之后

    * 类的属性

      * **loadFactor 加载因子**

        loadFactor 加载因子是控制数组存放数据的疏密程度，loadFactor 越趋近于 1，那么 数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加，loadFactor 越小，也就是趋近于 0，数组中存放的数据(entry)也就越少，也就越稀疏。

        **loadFactor 太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor 的默认值为 0.75f 是官方给出的一个比较好的临界值**。

        给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。

      * **threshold**

        **threshold = capacity \* loadFactor**，**当 Size>=threshold**的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 **衡量数组是否需要扩增的一个标准**。

    * 相比于之前的版本， JDK1.8 之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。

    * 当链表长度大于阈值（默认为 8）时，会首先调用 `treeifyBin()`方法。这个方法会根据 HashMap 数组来决定是否转换为红黑树。只有当数组长度大于或者等于 64 的情况下，才会执行转换红黑树操作，以减少搜索时间。否则，就是只是执行 `resize()` 方法对数组扩容。

    * TreeMap、TreeSet 以及 JDK1.8 之后的 HashMap 底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。
    
* 红黑树

  * 性质1：每个节点要么是黑色，要么是红色。

    性质2：根节点是黑色。

    性质3：每个叶子节点（NIL）是黑色。

    性质4：每个红色结点的两个子结点一定都是黑色。

    **性质5：任意一结点到每个叶子结点的路径都包含数量相同的黑结点。**


* 1.8 put方法

  * HashMap 只提供了 put 用于添加元素，putVal 方法只是给 put 方法调用的一个方法，并没有提供给用户使用。

    **对 putVal 方法添加元素的分析如下：**

    1. 如果定位到的数组位置没有元素 就直接插入。
    2. 如果定位到的数组位置有元素就和要插入的 key 比较，如果 key 相同就直接覆盖，直接覆盖之后应该就会 return，不会有后续操作
    3. 如果 key 不相同，就判断 p 是否是一个树节点，如果是就调用`e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value)`将元素添加进入。如果不是就遍历链表插入(插入的是链表尾部，1.8尾插法，1.7头插法)。
    4. 当链表长度大于阈值（默认为 8）并且 HashMap 数组长度超过 64 的时候才会执行链表转红黑树的操作，否则就只是对数组扩容。参考 HashMap 的 `treeifyBin()` 方法

* 1.7put方法

  * ① 如果定位到的数组位置没有元素 就直接插入。
  * ② 如果定位到的数组位置有元素，遍历以这个元素为头结点的链表，依次和插入的 key 比较，如果 key 相同就直接覆盖，不同就采用头插法插入元素。

* get方法
  
* 
  
* resize扩容机制

  * 触发扩容：

    * 一般情况下，**当元素数量超过阈值时**便会触发扩容。每次扩容的容量都是之前容量的2倍。

      HashMap的容量是有上限的，必须小于**1<<30**，即1073741824。如果容量超出了这个数，则不再增长，且阈值会被设置为**Integer.MAX_VALUE**（ ![[公式]](https://www.zhihu.com/equation?tex=2%5E%7B31%7D-1) ，即永远不会超出阈值了）。

  * 步骤：

    * 扩容：**创建一个新的Entry空数组，长度是原数组的2倍。**
    * ReHash：**遍历原Entry数组，把所有的Entry重新Hash到新数组。**
    * Hash的公式---> index = **HashCode（Key） & （Length - 1）**

  * 特性：

    * 空参数的构造函数：实例化的HashMap默认内部数组是null，即没有实例化。第一次调用put方法时，则会开始第一次初始化扩容，长度为16。
    * 有参构造函数：用于指定容量。会根据指定的正整数找到**不小于指定容量的2的幂数**，将这个数设置赋值给**阈值**（threshold）。第一次调用put方法时，会将阈值赋值给容量（因此并不是我们手动指定了容量就一定不会触发扩容，超过阈值后一样会扩容！！）
    * 如果不是第一次扩容，则容量变为原来的2倍，阈值也变为原来的2倍。*（容量和阈值都变为原来的2倍时，负载因子还是不变）*
    * 首次put时，**先会触发扩容（算是初始化）**，然后存入数据，然后判断是否需要扩容；
    * 不是首次put，则不再初始化，直接存入数据，然后判断是否需要扩容；

  * 扩容后元素迁移

    * JDK8则因为巧妙的设计，性能有了大大的提升：由于数组的容量是以**2的幂次方扩容**的，那么一个Entity在扩容时，新的位置要么在**原位置**，要么在**原长度+原位置**的位置。原因如下图：

    * 数组长度变为原来的2倍，表现在二进制上就是**多了一个高位参与数组下标确定**。此时，一个元素**通过hash转换坐标**的方法计算后，恰好出现一个现象：**最高位是0则坐标不变，最高位是1则坐标变为“10000+原坐标”**，即“原长度+原坐标”，如图：![image-20210326144633974](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210326144633974.png)

    * 因此，**在扩容时，不需要重新计算元素的hash了，只需要判断最高位是1还是0就好了。**

    * JDK8在迁移元素时是**正序**的，**不会出现链表转置的发生。**


### ConcurrentHashMap

* 和Hashtable区别

  * **底层数据结构：** JDK1.7 的 `ConcurrentHashMap` 底层采用 **分段的数组+链表** 实现，JDK1.8 采用的数据结构跟 `HashMap1.8` 的结构一样，数组+链表/红黑二叉树。`Hashtable` 和 JDK1.8 之前的 `HashMap` 的底层数据结构类似都是采用 **数组+链表** 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的；
  * **实现线程安全的方式（重要）：** 
  * ① **在 JDK1.7 的时候，`ConcurrentHashMap`（分段锁）** 对整个桶数组进行了分割分段(`Segment`)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 **到了 JDK1.8 的时候已经摒弃了 `Segment` 的概念，而是直接用 `Node` 数组+链表+红黑树的数据结构来实现，并发控制使用 `synchronized` 和 CAS 来操作。（JDK1.6 以后 对 `synchronized` 锁做了很多优化）** 整个看起来就像是优化过且线程安全的 `HashMap`，虽然在 JDK1.8 中还能看到 `Segment` 的数据结构，但是已经简化了属性，只是为了兼容旧版本；
  * ② **`Hashtable`(同一把锁)** :使用 `synchronized` 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。

* JDK1.8 的 `ConcurrentHashMap` 不在是 **Segment 数组 + HashEntry 数组 + 链表**，而是 **Node 数组 + 链表 / 红黑树**。不过，Node 只能用于链表的情况，红黑树的情况需要使用 **`TreeNode`**。当冲突链表达到一定长度时，链表会转换成红黑树。

* 线程安全实现

  * 1.7

    * jdk1.7 ConcurrentHashMap是由**一个Segment数组和多个HashEntry数组**组成

    * ![image-20210407215601663](https://gitee.com/picgo-table/picgo-img/raw/master/image/image-20210407215601663.png)

    * 首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。

      **`ConcurrentHashMap` 是由 `Segment` 数组结构和 `HashEntry` 数组结构组成**。

      Segment 实现了 `ReentrantLock`,所以 `Segment` 是一种可重入锁，扮演锁的角色。`HashEntry` 用于存储键值对数据。

    * 一个 `ConcurrentHashMap` 里包含一个 `Segment` 数组。`Segment` 的结构和 `HashMap` 类似，是一种数组和链表结构，一个 `Segment` 包含一个 `HashEntry` 数组，每个 `HashEntry` 是一个链表结构的元素，每个 `Segment`元素 守护着一个 `HashEntry` 数组里的元素，当对 `HashEntry` 数组的数据进行修改时，必须首先获得对应的 `Segment` 的锁。

  * 1.8

    * `ConcurrentHashMap` 取消了 `Segment` 分段锁，采用 CAS 和 `synchronized` 来保证并发安全。数据结构跟 HashMap1.8 的结构类似，数组+链表/红黑二叉树。Java 8 在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为 O(N)）转换为红黑树（寻址时间复杂度为 O(log(N))）

      `synchronized` **只锁定当前链表或红黑二叉树的首节点**，这样只要 hash 不冲突，就不会产生并发，效率又提升 N 倍。
      
    * **sizeCtl**变量(**volatile**修饰)
    
      通过**CAS操作+volatile**, 控制**数组初始化和扩容操作**
    

* 1.7底层

  * ConcurrnetHashMap 由很多个 Segment 组合，而每一个 Segment 是一个类似于 HashMap 的结构，所以每一个 HashMap 的内部可以进行扩容。但是 Segment 的个数一旦**初始化就不能改变**，默认 Segment 的个数是 16 个，你也可以认为 ConcurrentHashMap 默认支持最多 16 个线程并发。

  * 初始化

    * 必要参数校验。
    * 校验并发级别 concurrencyLevel 大小，如果大于最大值，重置为最大值。无惨构造**默认值是 16.**
    * 寻找并发级别 concurrencyLevel 之上最近的 **2 的幂次方**值，作为初始化容量大小，**默认是 16**。
    * 记录 segmentShift 偏移量，这个值为【容量 = 2 的N次方】中的 N，在后面 Put 时计算位置时会用到。**默认是 32 - sshift = 28**.
    * 记录 segmentMask，默认是 ssize - 1 = 16 -1 = 15.
    * **初始化 segments[0]**，**默认大小为 2**，**负载因子 0.75**，**扩容阀值是 2\*0.75=1.5**，插入第二个值时才会进行扩容。

  * put

    * 计算要 put 的 key 的位置，获取指定位置的 Segment。

    * 如果指定位置的 Segment 为空，则初始化这个 Segment.

      **初始化 Segment 流程：**

      1. 检查计算得到的位置的 Segment 是否为null.
      2. 为 null 继续初始化，使用 Segment[0] 的容量和负载因子创建一个 HashEntry 数组。
      3. 再次检查计算得到的指定位置的 Segment 是否为null.
      4. 使用创建的 HashEntry 数组初始化这个 Segment.
      5. 自旋判断计算得到的指定位置的 Segment 是否为null，使用 CAS 在这个位置赋值为 Segment.

    * Segment.put 插入 key,value 值。

    * 由于 Segment 继承了 **ReentrantLock**，所以 Segment 内部可以很方便的获取锁，put 流程就用到了这个功能。

      1. tryLock() 获取锁，获取不到使用 **`scanAndLockForPut`** 方法继续获取。

      2. 计算 put 的数据要放入的 index 位置，然后获取这个位置上的 HashEntry 。

      3. 遍历 put 新元素，为什么要遍历？因为这里获取的 HashEntry 可能是一个空元素，也可能是链表已存在，所以要区别对待。

         如果这个位置上的 **HashEntry 不存在**：

         1. 如果当前容量大于扩容阀值，小于最大容量，**进行扩容**。
         2. 直接头插法插入。

         如果这个位置上的 **HashEntry 存在**：

         1. 判断链表当前元素 Key 和 hash 值是否和要 put 的 key 和 hash 值一致。一致则替换值
         2. 不一致，获取链表下一个节点，直到发现相同进行值替换，或者链表表里完毕没有相同的。
            1. 如果当前容量大于扩容阀值，小于最大容量，**进行扩容**。
            2. 直接链表头插法插入。

      4. 如果要插入的位置之前已经存在，替换后返回旧值，否则返回 null.

      这里面的第一步中的 scanAndLockForPut 操作这里没有介绍，这个方法做的操作就是不断的自旋 `tryLock()` 获取锁。当自旋次数大于指定次数时，使用 `lock()` 阻塞获取锁。在自旋时顺表获取下 hash 位置的 HashEntry。

  * rehash

    * ConcurrentHashMap 的扩容只会扩容到原来的两倍。老数组里的数据移动到新的数组时，位置要么不变，要么变为 index+ oldSize，参数里的 node 会在扩容之后使用链表**头插法**插入到指定位置。

  * get

    * 计算得到 key 的存放位置。
    * 遍历指定位置查找相同 key 的 value 值。

* 1.8底层

  * 不再是之前的 **Segment 数组 + HashEntry 数组 + 链表**，而是 **Node 数组 + 链表 / 红黑树**。当冲突链表达到一定长度时，链表会转换成红黑树。

  * 初始化

    * 从源码中可以发现 ConcurrentHashMap 的初始化是通过**自旋和 CAS** 操作完成的。里面需要注意的是变量 `sizeCtl` ，它的值决定着当前的初始化状态。

    1. -1 说明正在初始化
    2. -N 说明有N-1个线程正在进行扩容
    3. 表示 table 初始化大小，如果 table 没有初始化
    4. 表示 table 容量，如果 table　已经初始化。

  * put

    * 根据 key 计算出 hashcode 。
    * 判断是否需要进行初始化。
    * 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，**利用 CAS 尝试写入，失败则自旋保证成功。**
    * 如果当前位置的 `hashcode == MOVED == -1`,则需要进行扩容。
    * 如果都不满足，则**利用 synchronized 锁写入数据。**
    * 如果数量大于 `TREEIFY_THRESHOLD` 则要转换为红黑树。

  * get 

    * 根据 hash 值计算位置。
    * 查找到指定位置，如果头节点就是要找的，直接返回它的 value.
    * 如果**头节点 hash 值小于 0** ，说明**正在扩容或者是红黑树**，查找之。
    * 如果是链表，遍历查找之。

* 7和8的区别

  * Java7 中 ConcruuentHashMap 使用的分段锁，也就是**每一个 Segment 上同时只有一个线程可以操作**，每一个 Segment 都是一个类似 HashMap 数组的结构，它可以扩容，它的冲突会转化为链表。但是 Segment 的个数**一但初始化就不能改变。**

    Java8 中的 ConcruuentHashMap 使用的 **Synchronized 锁加 CAS 的机制**。结构也由 Java7 中的 **Segment 数组 + HashEntry 数组 + 链表** 进化成了 **Node 数组 + 链表 / 红黑树**，Node 是类似于一个 HashEntry 的结构。它的冲突再达到一定大小时会转化成红黑树，在冲突小于一定数量时又退回链表。

### ArrayList

* 基本知识
  * `ArrayList` 的底层是数组队列，相当于动态数组。与 Java 中的数组相比，它的容量能**动态增长**。在添加大量元素前，应用程序可以使用`ensureCapacity`操作来增加 `ArrayList` 实例的容量。这可以**减少递增式再分配的数量**。
  * `RandomAccess` 是一个标志接口，表明实现这个这个接口的 List 集合是支持**快速随机访问**的。在 `ArrayList` 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。
  * `ArrayList` 实现了 **`Cloneable` 接口** ，即覆盖了函数`clone()`，能被克隆。
  * `ArrayList` 实现了 `java.io.Serializable `接口，这意味着`ArrayList`支持序列化，能通过序列化去传输。
* 与Vector区别
  * `ArrayList` 是 `List` 的主要实现类，底层使用 `Object[ ]`存储，适用于频繁的查找工作，线程不安全 ；
  * `Vector` 是 `List` 的古老实现类，底层使用 `Object[ ]`存储，线程安全的。
* 与LinkedList区别
  * **是否保证线程安全：** `ArrayList` 和 `LinkedList` 都是不同步的，也就是不保证线程安全；
  * **底层数据结构：** `Arraylist` 底层使用的是 **`Object` 数组**；`LinkedList` 底层使用的是 **双向链表** 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！）
  * **插入和删除是否受元素位置的影响：** 
    * ① **`ArrayList` 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。** 比如：执行`add(E e)`方法的时候， `ArrayList` 会默认在将指定的元素**追加到此列表的末尾**，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（`add(int index, E element)`）时间复杂度就为 **O(n-i)**。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 
    * ② **`LinkedList` 采用链表存储，所以对于`add(E e)`方法的插入，删除元素时间复杂度不受元素位置的影响，近似 O(1)，如果是要在指定位置`i`插入和删除元素的话（`(add(int index, E element)`） 时间复杂度近似为`o(n))`因为需要先移动到指定位置再插入。**
* **是否支持快速随机访问：** `LinkedList` 不支持高效的随机元素访问，而 `ArrayList` 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于`get(int index)`方法)。
  
  * **内存空间占用：** `ArrayList` 的空 间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 `LinkedList` 的空间花费则体现在它的每一个元素都需要消耗比 `ArrayList` 更多的空间（因为要存放直接后继和直接前驱以及数据）。
  
* 扩容机制

  * **ArrayList 有三种方式来初始化**: **以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。**

  * add方法：

    * 确保内部容量足够size + 1,size为当前Arraylist元素的个数
    * 将元素放到对应的下标
    * 返回true、
    * ensureCapacityInternal方法与ensureExplicitCapacity是确保数组扩容到指定的大小

  * grow方法

    ```java
        private void grow(int minCapacity) {
            //之前的容量
            int oldCapacity = elementData.length;
            //新的容量为之前的容量 1.5倍
            int newCapacity = oldCapacity + (oldCapacity >> 1);
            //如果新的容量小于 要扩容的容量，新的容量等于要扩容的容量
            if (newCapacity - minCapacity < 0)
                newCapacity = minCapacity;
            //如果已经大于了最大的容量，那么已经到了最大的大小            
            if (newCapacity - MAX_ARRAY_SIZE > 0)
                newCapacity = hugeCapacity(minCapacity);
            // minCapacity is usually close to size, so this is a win:
            elementData = Arrays.copyOf(elementData, newCapacity);
        }
    ```

    * 简单来说就是在增长数组的时候，与**所需的最小的容量**进行比较
    * 保证要扩容的大小**大于最小满足的容量**
    * 如果已经大于了最大的数组大小，再做一次最大的容量处理









